DATABASE_NAME=
DATABASE_USER=
DATABASE_PASSWORD=
DATABASE_HOST=
DATABASE_PORT=

# Sections Database (for dynamic section content tables)
# Optional - defaults to Docker Compose sections_db service if not set
SECTIONS_DB_NAME=
SECTIONS_DB_USER=
SECTIONS_DB_PASSWORD=
SECTIONS_DB_HOST=
SECTIONS_DB_PORT=

# Lambda-specific database URLs (for AWS Lambda functions)
# If set, these will be used for Lambda environment variables instead of constructing from individual settings
# Example: postgresql://user:password@rds-instance.region.rds.amazonaws.com:5432/dbname
LAMBDA_DATABASE_URL=
LAMBDA_SECTIONS_DATABASE_URL=

# Redis Configuration
# For self-hosted: redis://redis:6379 (local Docker Redis)
# For cloud: redis://:password@your-redis-host.com:6379
REDIS_URL=redis://redis:6379

# ============================================================================
# Authentication Configuration
# ============================================================================
# SAAS_MODE controls which authentication system to use:
#   true  = AWS Cognito (cloud-based, for SaaS deployments)
#   false = Standalone FastAPI auth (self-hosted, for local/private deployments)
SAAS_MODE=true

# Cognito Settings (required when SAAS_MODE=true)
COGNITO_AWS_REGION=
COGNITO_USER_POOL_ID=
COGNITO_APP_CLIENT_ID=

# Standalone Auth Settings (required when SAAS_MODE=false)
# Generate JWT_SECRET_KEY with: openssl rand -hex 32
JWT_SECRET_KEY=
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

AWS_REGION=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

AWS_ACM_CERT_ARN=
AWS_LAMBDA_EXECUTION_ROLE=
AWS_LAMBDA_LAYER_ARN=
AWS_ACCOUNT_ID=

AWS_S3_PUBLIC_BUCKET_NAME=
AWS_S3_PRIVATE_BUCKET_NAME=

EMAIL_HOST_USER=
EMAIL_HOST_PASSWORD=
EMAIL_FROM=no-reply@polysynergy.com

PORTAL_URL=
ROUTER_URL=

DEBUG=

EXECUTE_NODE_SETUP_LOCAL=True

DYNAMODB_ENV_VARS_TABLE=polysynergy_env_vars

OPENAI_API_KEY=

# Sentry Error Tracking (optional)
# Get your DSN from: https://sentry.io/settings/[your-org]/projects/[your-project]/keys/
SENTRY_DSN=
# Environment name that shows in Sentry (development/staging/production)
SENTRY_ENVIRONMENT=development

# ============================================================================
# Self-Hosted Configuration
# ============================================================================
# For self-hosted deployments (running on your own infrastructure):
# - Set these endpoints to use local services (DynamoDB Local, MinIO)
# - Leave empty to use AWS cloud services

# DynamoDB Local endpoint (leave empty for AWS DynamoDB)
# Example: http://dynamodb-local:8000 (Docker) or http://localhost:8001 (host machine)
DYNAMODB_LOCAL_ENDPOINT=http://dynamodb-local:8000

# MinIO S3-compatible storage (leave empty for AWS S3)
# Example: http://minio:9000 (Docker) or http://localhost:9000 (host machine)
S3_LOCAL_ENDPOINT=http://minio:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin

# Routes table name (used by router service)
ROUTES_TABLE_NAME=polysynergy_routes

# ============================================================================
# Encryption Configuration (for DynamoDB secrets)
# ============================================================================
# This key encrypts/decrypts sensitive data (API keys, passwords) stored in DynamoDB
# Generate a new key with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
#
# IMPORTANT:
# - Each environment should use its own encryption key
# - Self-hosted: Generate your own key
# - AWS/Cloud: Use separate production key
# - NEVER commit production keys to git!
# - Store production key in secure vault (1Password, AWS Secrets Manager, etc.)
# - Rotate keys periodically
#
# Example self-hosted key: XQyy0SiwKsDFPRZ4Ute2bAu8ArCno3opMOOYssfVViM=
ENCRYPTION_KEY=

# Keys for migration script (if migrating from AWS to self-hosted)
AWS_ENCRYPTION_KEY=
LOCAL_ENCRYPTION_KEY=

# ============================================================================
# Optional: Self-Hosted AI with Ollama
# ============================================================================
# Uncomment if using docker-compose.ollama.yml for local LLM inference
# This allows ModelOllama nodes to connect to containerized Ollama service
# Default uses Docker internal network (ollama:11434)
#
# OLLAMA_HOST=http://ollama:11434